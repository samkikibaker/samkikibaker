---
title: "Correct Technique: How machine learning can ensure proper form in your workout"
date: 2019-09-09
tags: [Machine Learning, Data Cleaning]
header:
  image: "/images/Exercise images/exercise.jpg"
excerpt: "Machine Learning, Data Cleaning"
---

Introduction
============

The Jawbone Up, Nike FuelBand, and Fitbit are all examples of wearable
technology that can collect a large amount of data about personal
activity relatively inexpensively. These devices are part of the
'quantified self' movement - a group of enthusiasts who take
measurements about themselves, often with the goal of improving their
health. Such devices are usually able to quantify how much of a
particular activity is done, but they rarely quantify how well it is
performed. Thus there is scope for expanding the functionality of these
devices to incorporate this capability.

Aims and Data
=============

An experiment was performed in which six young health males (20-28) were
asked to perform one set of ten repetitions of the unilateral dumbbell
biceps curl in five different fashions. The first method was the correct
form, whilst the other four methods corresponded to four common errors:

-   Class A - Exactly according to the specification  
-   Class B - Throwing the elbows to the front  
-   Class C - Lifting the dumbbell only halfway  
-   Class D - Lowering the dumbbell only halfway  
-   Class E - Throwing the hips to the front

Data was collected from accelerometers on the belt, forearm, arm and
dumbell of the six participants. The aim of this project will be to use
this data to build a model capable of categorising which of the five
classes a particular set is in.

Data Cleaning
=============

We begin by loading the necessary packages and downloading the data into
the working directory.

    #Load packages
    require(caret, warn.conflicts = FALSE, quietly = TRUE)
    require(dplyr, warn.conflicts = FALSE, quietly = TRUE)
    require(rattle, warn.conflicts = FALSE, quietly = TRUE)

    ## Rattle: A free graphical interface for data science with R.
    ## Version 5.2.0 Copyright (c) 2006-2018 Togaware Pty Ltd.
    ## Type 'rattle()' to shake, rattle, and roll your data.

    #Download data into working directory and read into R.
    path <- getwd()
    urltrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    urltest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(urltrain, file.path(path, "Train Data.csv"))
    download.file(urltest, file.path(path, "Test Data.csv"))
    rawtrain <- read.csv("Train Data.csv", na.strings = c("#DIV/0!", "NA"))
    rawtest <- read.csv("Test Data.csv", na.strings = c("#DIV/0!", "NA"))

Let's take a look at the dimensions of the data.

    dim(rawtrain)

    ## [1] 19622   160

    dim(rawtest)

    ## [1]  20 160

We see that the data contains 160 variables with 19622 observations in
the training set and 20 observations in the test test.

The first seven columns do not contain information from the
accelerometers so these are dropped.

    data <- rawtrain[, -c(1:7)]

We next examine the proportion of missing values in the data. In
particular we look at variables which have less than 30% missing values.

    #The proportion of missing values within each column
    percNA <- sapply(data, function(x){sum(is.na(x))/length(x)})

    # The number of variables with less than 30% missing values.
    vars <- percNA < 0.30
    sum(vars)

    ## [1] 53

There are only 53 columns with less than 30% missing values, only these
variables are kept as the others had too little information to be of any
use in prediction.

    data <- data[, vars]

Finally the remaining variables do not have any missing values, so
imputation is not required.

    #The total number of missing values within the 53 remaining variables
    sum(is.na(data))

    ## [1] 0

Modelling
=========

Having cleaned the data, we next partition the training set further into
a validation and training set. We use a 70% - 30% split between the
training and validation sets. The seed is set for reproducibility.

    set.seed(864)
    partition <- createDataPartition(data$classe, p = 0.7, list = FALSE)
    training <- data[partition,]
    validation <- data[-partition,]

We will now use three different machine learning algorithms to fit
models to the training data. Namely, we will use decision trees, random
forests and gradient boosting. The model which performs best on the
validation set will be chosen as the final model to be evaluated on the
test set.

Decision Trees
--------------

We apply the recursive partitioning (rpart) algorithm and use ten-fold
cross-validation repeated three times to build thirty decision trees.
The best one is evaluated on the validation set and its accuracy is
reported.

    train.control1 <- trainControl(method = "repeatedcv",
    number = 10,
    repeats = 3)
    model1 <- train(training[, -53], training$classe,
    method = "rpart",
    trControl = train.control1)

    trainpred1 <- predict(model1, validation)
    confMat1 <- confusionMatrix(validation$classe, trainpred1)
    confMat1

    ## Confusion Matrix and Statistics
    ##
    ##           Reference
    ## Prediction    A    B    C    D    E
    ##          A 1036    6  317  309    6
    ##          B  180  196  204  559    0
    ##          C   29   27  848  122    0
    ##          D   56    9  289  610    0
    ##          E   21    4  227  337  493
    ##
    ## Overall Statistics
    ##                                          
    ##                Accuracy : 0.5409         
    ##                  95% CI : (0.528, 0.5537)
    ##     No Information Rate : 0.3291         
    ##     P-Value [Acc > NIR] : < 2.2e-16      
    ##                                          
    ##                   Kappa : 0.4281         
    ##                                          
    ##  Mcnemar's Test P-Value : < 2.2e-16      
    ##
    ## Statistics by Class:
    ##
    ##                      Class: A Class: B Class: C Class: D Class: E
    ## Sensitivity            0.7837  0.80992   0.4499   0.3149  0.98798
    ## Specificity            0.8602  0.83289   0.9555   0.9103  0.89064
    ## Pos Pred Value         0.6189  0.17208   0.8265   0.6328  0.45564
    ## Neg Pred Value         0.9321  0.99031   0.7866   0.7303  0.99875
    ## Prevalence             0.2246  0.04112   0.3203   0.3291  0.08479
    ## Detection Rate         0.1760  0.03331   0.1441   0.1037  0.08377
    ## Detection Prevalence   0.2845  0.19354   0.1743   0.1638  0.18386
    ## Balanced Accuracy      0.8219  0.82140   0.7027   0.6126  0.93931

We see that this algorithm produces an accuracy of 0.5408666 on the
validation set.

Random Forests
--------------

We next apply the random forests (rf) algorithm and use five-fold
cross-validation. The best model is evaluated on the validation set and
its accuracy reported on.

    train.control2 <- trainControl(method = "repeatedcv",
    number = 5,
    repeats = 1)
    model2 <- train(training[, -53], training$classe,
    method = "rf", ntree = 100,
    trControl = train.control2)

    trainpred2 <- predict(model2, validation)
    confMat2 <- confusionMatrix(validation$classe, trainpred2)
    confMat2

    ## Confusion Matrix and Statistics
    ##
    ##           Reference
    ## Prediction    A    B    C    D    E
    ##          A 1671    2    0    0    1
    ##          B   10 1127    2    0    0
    ##          C    0    1 1023    2    0
    ##          D    0    2   10  951    1
    ##          E    0    0    3    6 1073
    ##
    ## Overall Statistics
    ##                                           
    ##                Accuracy : 0.9932          
    ##                  95% CI : (0.9908, 0.9951)
    ##     No Information Rate : 0.2856          
    ##     P-Value [Acc > NIR] : < 2.2e-16       
    ##                                           
    ##                   Kappa : 0.9914          
    ##                                           
    ##  Mcnemar's Test P-Value : NA              
    ##
    ## Statistics by Class:
    ##
    ##                      Class: A Class: B Class: C Class: D Class: E
    ## Sensitivity            0.9941   0.9956   0.9855   0.9917   0.9981
    ## Specificity            0.9993   0.9975   0.9994   0.9974   0.9981
    ## Pos Pred Value         0.9982   0.9895   0.9971   0.9865   0.9917
    ## Neg Pred Value         0.9976   0.9989   0.9969   0.9984   0.9996
    ## Prevalence             0.2856   0.1924   0.1764   0.1630   0.1827
    ## Detection Rate         0.2839   0.1915   0.1738   0.1616   0.1823
    ## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
    ## Balanced Accuracy      0.9967   0.9965   0.9925   0.9945   0.9981

We see that this algorithm produces an accuracy of 0.9932031 on the
validation set. We might have considered using more folds for cross
validation, repeating the cross-validation process and tuning more of
the parameters, however this would increase run time dramatically. Since
the model already performs very well, it was decided that this was
unnecessary.

Gradient Boosting
-----------------

Finally we apply the extreme gradient boosting (xgbTree) algorithm and
use three-fold cross-validation. This is not repeated due to long run
time. The best model is evaluated on the validation set and its accuracy
reported on.

    train.control3 <- trainControl(method = "repeatedcv",
    number = 3,
    repeats = 1)
    model3 <- train(training[, -53], training$classe,
    method = "xgbTree",
    trControl = train.control3)

    trainpred3 <- predict(model3, validation)
    confMat3 <- confusionMatrix(validation$classe, trainpred3)
    confMat3

    ## Confusion Matrix and Statistics
    ##
    ##           Reference
    ## Prediction    A    B    C    D    E
    ##          A 1669    4    1    0    0
    ##          B    5 1130    4    0    0
    ##          C    0    0 1024    2    0
    ##          D    0    1    9  954    0
    ##          E    0    0    2    2 1078
    ##
    ## Overall Statistics
    ##                                           
    ##                Accuracy : 0.9949          
    ##                  95% CI : (0.9927, 0.9966)
    ##     No Information Rate : 0.2845          
    ##     P-Value [Acc > NIR] : < 2.2e-16       
    ##                                           
    ##                   Kappa : 0.9936          
    ##                                           
    ##  Mcnemar's Test P-Value : NA              
    ##
    ## Statistics by Class:
    ##
    ##                      Class: A Class: B Class: C Class: D Class: E
    ## Sensitivity            0.9970   0.9956   0.9846   0.9958   1.0000
    ## Specificity            0.9988   0.9981   0.9996   0.9980   0.9992
    ## Pos Pred Value         0.9970   0.9921   0.9981   0.9896   0.9963
    ## Neg Pred Value         0.9988   0.9989   0.9967   0.9992   1.0000
    ## Prevalence             0.2845   0.1929   0.1767   0.1628   0.1832
    ## Detection Rate         0.2836   0.1920   0.1740   0.1621   0.1832
    ## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
    ## Balanced Accuracy      0.9979   0.9968   0.9921   0.9969   0.9996

We see that this algorithm produces an accuracy of 0.9949023 on the
validation set. We might have considered using more folds for cross
validation or repeating the cross-validation process, however this would
increase run time dramatically. The model already performs very well, so
this was unnecessary.

Final Model
===========

We observe that the gradient boosting algorithm performed the best on the
validation set with an estimated out of sample error rate of 0.0050977.
We now evaluate this model on the original test set consisting of 20
observations.

We start by extracting the variables on which our model was build from
the test dataset.

    variables <- colnames(training)[1:52]
    test <- select(rawtest, variables)

Finally we use the gradient boosting model to predict values for the
remaining twenty samples.

    finalPred <- predict(model3, test)
    data.frame(FinalPrediction = finalPred)

    ##    FinalPrediction
    ## 1                B
    ## 2                A
    ## 3                B
    ## 4                A
    ## 5                A
    ## 6                E
    ## 7                D
    ## 8                B
    ## 9                A
    ## 10               A
    ## 11               B
    ## 12               C
    ## 13               B
    ## 14               A
    ## 15               E
    ## 16               E
    ## 17               A
    ## 18               B
    ## 19               B
    ## 20               B
